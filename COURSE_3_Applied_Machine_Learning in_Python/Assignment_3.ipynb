{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.1** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-machine-learning/resources/bANLa) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 - Evaluation\n",
    "\n",
    "In this assignment you will train several models and evaluate how effectively they predict instances of fraud using data based on [this dataset from Kaggle](https://www.kaggle.com/dalpozz/creditcardfraud).\n",
    " \n",
    "Each row in `fraud_data.csv` corresponds to a credit card transaction. Features include confidential variables `V1` through `V28` as well as `Amount` which is the amount of the transaction. \n",
    " \n",
    "The target is stored in the `class` column, where a value of 1 corresponds to an instance of fraud and 0 corresponds to an instance of not fraud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Import the data from `fraud_data.csv`. What percentage of the observations in the dataset are instances of fraud?\n",
    "\n",
    "*This function should return a float between 0 and 1.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    # Your code here\n",
    "    fraud_df=pd.read_csv('fraud_data.csv')\n",
    "    fraud_class_percentage=fraud_df['Class'].sum()/len(fraud_df['Class'])\n",
    "    \n",
    "    return fraud_class_percentage# Return your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Using `X_train`, `X_test`, `y_train`, and `y_test` (as defined above), train a dummy classifier that classifies everything as the majority class of the training data. What is the accuracy of this classifier? What is the recall?\n",
    "\n",
    "*This function should a return a tuple with two floats, i.e. `(accuracy score, recall score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_two():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import accuracy_score,recall_score\n",
    "    \n",
    "    \n",
    "    # Your code here\n",
    "    # dummy classifier that classifies everything as the majority class of the training data\n",
    "    # use most frequent strategy always predicts '0'\n",
    "      \n",
    "    dclf = DummyClassifier(strategy = 'most_frequent', random_state = 0)\n",
    "    dclf.fit(X_train, y_train)\n",
    "    \n",
    "    #making predictions \n",
    "    \n",
    "    y_pred = dclf.predict(X_test)\n",
    "    \n",
    "    return (accuracy_score(y_test, y_pred)),(recall_score(y_test, y_pred))# Return your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Using X_train, X_test, y_train, y_test (as defined above), train a SVC classifer using the default parameters. What is the accuracy, recall, and precision of this classifier?\n",
    "\n",
    "*This function should a return a tuple with three floats, i.e. `(accuracy score, recall score, precision score)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_three():\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "    from sklearn import svm\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Your code here\n",
    "    # Default Parameters\n",
    "    c_value = 1.0\n",
    "    krenel_ = 'rbf'\n",
    "    \n",
    "    # Define our classifier and fit our data\n",
    "    SVC_ = svm.SVC(kernel=krenel_, C = c_value, random_state = 0).fit(X_train, y_train)\n",
    "    \n",
    "    #get predictions\n",
    "    y_pred = SVC_.predict(X_test)\n",
    "    \n",
    "\n",
    "    return (accuracy_score(y_test, y_pred)),(recall_score(y_test, y_pred)),(precision_score(y_test, y_pred))# Return your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Using the SVC classifier with parameters `{'C': 1e9, 'gamma': 1e-07}`, what is the confusion matrix when using a threshold of -220 on the decision function. Use X_test and y_test.\n",
    "\n",
    "*This function should return a confusion matrix, a 2x2 numpy array with 4 integers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_four():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn import svm\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Your code here\n",
    "    # Default Parameters\n",
    "    c_value = 1e9\n",
    "    gamma_ = 1e-07\n",
    "    \n",
    "    # Define our classifier and fit our data\n",
    "    SVC_ = svm.SVC(C = c_value, gamma=gamma_).fit(X_train, y_train)\n",
    "\n",
    "    # Using Decision Function Method Present in svc class\n",
    "    Decision_Function = SVC_.decision_function(X_test)\n",
    "    \n",
    "    #predictions with decision function and threshold\n",
    "    y_pred_threshold = Decision_Function > (-220)\n",
    "\n",
    "    # confusion matrix\n",
    "    confusion_matrix = confusion_matrix(y_test, y_pred_threshold)\n",
    "    \n",
    "    return confusion_matrix# Return your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Train a logisitic regression classifier with default parameters using X_train and y_train.\n",
    "\n",
    "For the logisitic regression classifier, create a precision recall curve and a roc curve using y_test and the probability estimates for X_test (probability it is fraud).\n",
    "\n",
    "Looking at the precision recall curve, what is the recall when the precision is `0.75`?\n",
    "\n",
    "Looking at the roc curve, what is the true positive rate when the false positive rate is `0.16`?\n",
    "\n",
    "*This function should return a tuple with two floats, i.e. `(recall, true positive rate)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "\n",
    "    \n",
    "    # Define our classifier and fit our data\n",
    "    logreg = LogisticRegression(max_iter=100000)\n",
    "    logreg_ = logreg.fit(X_train, y_train)\n",
    "    \n",
    "    #get predictions\n",
    "    y_pred = logreg_.predict(X_test)\n",
    "    \n",
    "    # calculate model precision recall scores\n",
    "    precision_scores, recall_scores, _ = precision_recall_curve(y_test, y_pred)\n",
    "    \n",
    "    #get tpr and fpr\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "    \n",
    "    recall = np.interp(0.75, precision_scores, recall_scores).round(2)\n",
    "    true_positive_rate = np.interp(0.16, fpr, tpr).round(2)\n",
    "    \n",
    "    return recall, true_positive_rate# Return your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_five_plot():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    from sklearn.metrics import roc_auc_score, roc_curve\n",
    "    from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "    \n",
    "    #%matplotlib notebook\n",
    "\n",
    "    # Define our classifier and fit our data\n",
    "    logreg = LogisticRegression()\n",
    "    logreg_ = logreg.fit(X_train, y_train)\n",
    "    \n",
    "    #get predictions\n",
    "    y_pred = logreg_.predict(X_test)\n",
    "    \n",
    "    # calculate model precision recall scores\n",
    "    precision_scores, recall_scores, _ = precision_recall_curve(y_test, y_pred)\n",
    "    \n",
    "    #get tpr and fpr\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\n",
    "    \n",
    "    # Draw precision recall curve and roc curve\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(7,4))\n",
    "\n",
    "    #Precision recall curve plot\n",
    "    logit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\n",
    "    ax1.plot(fpr, tpr, label='ROC (area = %0.2f)' % logit_roc_auc, color='violet', lw=1.5)\n",
    "    ax1.plot([0, 1], [0, 1], color='blue', lw=1.5, linestyle='--')\n",
    "    ax1.set_xlim([-0.01, 1])\n",
    "    ax1.set_ylim([-0.01, 1])\n",
    "    ax1.set_xlabel('False Positive Rate', size=10)\n",
    "    ax1.set_ylabel('True Positive Rate', size=10)\n",
    "    ax1.set_title('Receiver operating characteristic', size=10)\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    \n",
    "    # Plot precision-recall curve\n",
    "\n",
    "    ax2.plot(recall_scores, precision_scores, color='limegreen', lw=1.5)\n",
    "    ax2.set_xlabel('Recall', size=10)\n",
    "    ax2.set_ylabel('Precision', size=10)\n",
    "    ax2.set_title('precision-recall curv', size=10);\n",
    "\n",
    "    recall = np.interp(0.75, precision_scores, recall_scores).round(2)\n",
    "    true_positive_rate = np.interp(0.16, fpr, tpr).round(2)\n",
    "    \n",
    "    #mark the points on graphs\n",
    "    ax1.plot(0.16, true_positive_rate,  ls=\"\", marker=\"*\", ms=7,  color=\"crimson\")\n",
    "    ax2.plot(0.75, recall, ls=\"\", marker=\"*\", ms=7,  color=\"crimson\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return fig.tight_layout()# Return your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "Perform a grid search over the parameters listed below for a Logisitic Regression classifier, using recall for scoring and the default 3-fold cross validation.\n",
    "\n",
    "`'penalty': ['l1', 'l2']`\n",
    "\n",
    "`'C':[0.01, 0.1, 1, 10, 100]`\n",
    "\n",
    "From `.cv_results_`, create an array of the mean test scores of each parameter combination. i.e.\n",
    "\n",
    "|      \t| `l1` \t| `l2` \t|\n",
    "|:----:\t|----\t|----\t|\n",
    "| **`0.01`** \t|    ?\t|   ? \t|\n",
    "| **`0.1`**  \t|    ?\t|   ? \t|\n",
    "| **`1`**    \t|    ?\t|   ? \t|\n",
    "| **`10`**   \t|    ?\t|   ? \t|\n",
    "| **`100`**   \t|    ?\t|   ? \t|\n",
    "\n",
    "<br>\n",
    "\n",
    "*This function should return a 5 by 2 numpy array with 10 floats.* \n",
    "\n",
    "*Note: do not return a DataFrame, just the values denoted by '?' above in a numpy array.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_six():    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Your code here\n",
    "    # Define our classifier and fit our data\n",
    "    logreg = LogisticRegression(C=1, penalty='l1', solver='liblinear')\n",
    "    \n",
    "    grid_params = {'penalty': ['l1', 'l2'],'C':[0.01, 0.1, 1, 10, 100]}\n",
    "    \n",
    "    gs_logreg = GridSearchCV(logreg, param_grid=grid_params, scoring='recall')\n",
    "    gs_logreg.fit(X_train, y_train)\n",
    "    \n",
    "    cv_result = pd.DataFrame(gs_logreg.cv_results_)\n",
    "    \n",
    "    mean_test_score = np.array(cv_result['mean_test_score']).reshape(5,2)\n",
    "    \n",
    "    return mean_test_score# Return your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the following function to help visualize results from the grid search\n",
    "def GridSearch_Heatmap(scores):\n",
    "    #%matplotlib notebook\n",
    "    plt.figure()\n",
    "    sns.heatmap(scores.reshape(5,2), xticklabels=['l1','l2'], yticklabels=[0.01, 0.1, 1, 10, 100])\n",
    "    plt.yticks(rotation=0);\n",
    "    return plt.show()\n",
    "#GridSearch_Heatmap(answer_six())"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-machine-learning",
   "graded_item_id": "5yX9Z",
   "launcher_item_id": "eqnV3",
   "part_id": "Msnj0"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
